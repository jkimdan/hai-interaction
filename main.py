from fastapi import FastAPI, HTTPException, Request, UploadFile
from fastapi.staticfiles import StaticFiles
from starlette.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from openai import OpenAI
import openai
import os
from dotenv import load_dotenv
import json
from typing import List, Dict, Any
import sys, re
from io import StringIO
import pandas as pd


# Load environment variables from .env file
load_dotenv()

app = FastAPI()

# Mount the static directory
app.mount("/static", StaticFiles(directory="static"), name="static")

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Adjust this to restrict allowed origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Load OpenAI API key from environment variable
client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

# Define request and response models
class QueryRequest(BaseModel):
    prompt: str
    dataset_info: List[Dict[str, Any]]
    file_url: str

class APIResponse(BaseModel):
    spec: dict = None
    description: str
class QueryResponse(BaseModel):
    response: APIResponse

def sanitize_input(query: str) -> str:
    """Sanitize input to the python REPL.
    Remove whitespace, backtick & python (if llm mistakes python console as terminal
    """

    # Removes `, whitespace & python from start
    query = re.sub(r"^(\s|`)*(?i:python)?\s*", "", query)
    # Removes whitespace & ` from end
    query = re.sub(r"(\s|`)*$", "", query)
    query = query.replace("\\n", "\n")
    return query
    
def execute_panda_dataframe_code(code, file_path):
    """
    Execute the given python code and return the output. 
    References:
    1. https://github.com/langchain-ai/langchain-experimental/blob/main/libs/experimental/langchain_experimental/utilities/python.py
    2. https://github.com/langchain-ai/langchain-experimental/blob/main/libs/experimental/langchain_experimental/tools/python/tool.py
    """
     # Save the current standard output to restore later
    old_stdout = sys.stdout
    # Redirect standard output to a StringIO object to capture any output generated by the code execution
    sys.stdout = mystdout = StringIO()
    try:
		    # Execute the provided code within the current environment
        cleaned_command = sanitize_input(code)
        df = pd.read_csv(file_path)
        exec_globals = {'df': df}
        exec(code, exec_globals)
        
        # Restore the original standard output after code execution
        sys.stdout = old_stdout
				
				# Return any captured output from the executed code
        return mystdout.getvalue()
    except Exception as e:
        sys.stdout = old_stdout
        return repr(e)

execute_panda_dataframe_code_tool = {
    "type": "function",
    "function": {
        "name": "execute_panda_dataframe_code",
        "description": '''Executes Python code related to Pandas DataFrame operations and returns the output. Ensure that any output you want to see is printed using the print() function.
        Assume the dataframe is already stored in a variable called df and that pandas is already imported''',
        "parameters": {
            "type": "object",
            "properties": {
                "code": {
                    "type": "string",
                    "description": '''The pandas code to get requested information. Assume the dataframe is already stored in a variable called df and that pandas is already imported.
                    Example: to compute the median miles per gallon (mpg) of European cars from the 'cars' dataset, code would be 
                    code = """
                    european_cars = cars[cars['origin'] == 'Europe']
                    median_mpg = european_cars['mpg'].median()
                    print(median_mpg)
                    """
                    '''
                },
                "file_path": {
                    "type":"string",
                    "description": "the local path to the csv file on the server."
                }
            },
            "required": ["code", "file_path"],
            "additionalProperties": False
        }
    }
}
def generate_vega_spec(vega_spec, data_url=None):
    try:
            # Handle any unnecessary escape sequences
            vega_spec = vega_spec.replace("\\n", "\n").replace('\\"', '"')
            vega_spec = json.loads(vega_spec)
    except json.JSONDecodeError as e:
        raise ValueError(f"Failed to parse Vega-Lite specification: {e}")
    if data_url and "values" not in vega_spec.get("data", {}):
        vega_spec["data"] = {
            "url": data_url,
            "format": {
                "type": "csv"
            }
        }
    return json.dumps(vega_spec, indent=2)

generate_vega_spec_tool = {
    "type": "function",
    "function": {
        "name": "generate_vega_spec",
        "description": "Generate a Vega-Lite specification for visualizing data based on user query.",
        "parameters": {
            "type": "object",
            "properties": {
                "vega_spec": {
                    "type": "string",
                    "description": "The well designed VegaLite specification to be rendered."
                },
                "data_url": {
                    "type": "string",
                    "description":"The URL to the CSV file."
                }
            },
            "required": ["vega_spec"],
            "additionalProperties": False
        }
    }
}

tools = [generate_vega_spec_tool, execute_panda_dataframe_code_tool]
tool_map = {
    "generate_vega_spec": generate_vega_spec,
    "execute_panda_dataframe_code": execute_panda_dataframe_code
}


@app.post("/query", response_model=QueryResponse)
async def query_openai(request: QueryRequest):
    try:
        dataset_info = request.dataset_info
        file_url = request.file_url
        file_path = file_url.replace("https://hai-interaction.onrender.com/", "")

        if not dataset_info:
            return QueryResponse(response=APIResponse(description="Please upload a dataset before sending any messages."))
        dataset_info_str = "\n".join(
            f"Column: {col['column_name']}\nType: {col['data_type']}\n"
            for col in dataset_info
        )
        
        # Construct the messages
        messages = [
            {
                "role": "system",
                "content": (
                    f'''
                    You are a data explorer assistant that generates VegaLite specifications and insights about a CSV based on user queries, which is rendered client-side to the user.
                    Only generate charts when it is relevant.             
                    You have access to the following tools:
                    {tools}

                    Question: the input question you must answer

                    Thought: you should always think about what to do
                    Action: the tool name, should be one of [{tool_map.keys()}]. If no tool need, just output "no tool"
                    Action Input: the input to the tool in a json format ({{"arg name": "arg value"}}). Otherwise, empty json object {{}}


                    You will return this action and action input, then wait for the Observation.

                    You will be then call again with the result of the action.

                    Observation: the result of the action
                    ... (this Thought/Action/Action Input/Observation can repeat N times)
                    Thought: I now know the final answer
                    Final Answer: the final answer to the original input question

                    Please ALWAYS start with a Thought. Self correct your function calling if there is an issue at each iteration.

                    
                    Here's an example, for a query about a CSV containing information about cars, if a user asked: 
                    Create a bar chart comparing the average mpg across different origins, and provide the numerical averages.
                    You would first calculate the average mpgs across the different origins, and then use the VegaLite tool to graph your results.
                    
                    If the query is irrelevant or unanswerable based on the dataset, politely explain to the user that you cannot fulfill the request.
                    For example: The dataset is related to cars. The user asks: How are you? You might respond: 'How are you' is not relevant to the dataset, which contains information about cars and their specifications.
                    '''
                )
            },
            {
                "role": "system",
                "content": f"Dataset Information:\n{dataset_info_str}\n File URL: {file_url} \n File Path: {file_path}"
            },
            {
                "role": "user",
                "content": request.prompt
            }
        ]

        vega_spec = None
        i=0
        max_iterations = 10
        while i < max_iterations:
            i += 1
            print("iteration:", i)
            response = client.chat.completions.create(
                model="gpt-4o-mini", temperature=0.0, messages=messages, tools=tools
            )
            # print(response.choices[0].message)
            if response.choices[0].message.content != None:
                print_red(response.choices[0].message.content)
            # print(response.choices[0].message)

            # if not function call
            if response.choices[0].message.tool_calls == None:
                break

            # if function call
            messages.append(response.choices[0].message)
            for tool_call in response.choices[0].message.tool_calls:
                print_blue("calling:", tool_call.function.name, "with", tool_call.function.arguments)
                # call the function
                arguments = json.loads(tool_call.function.arguments)
                function_to_call = tool_map[tool_call.function.name]
                result = function_to_call(**arguments)

                if tool_call.function.name == "generate_vega_spec":
                    try:
                        print(vega_spec)
                        vega_spec = json.loads(result)
                    except json.JSONDecodeError as e:
                        raise ValueError("Generated Vega-Lite specification is not valid JSON.") from e

                # create a message containing the result of the function call
                result_content = json.dumps({**arguments, "result": result})
                function_call_result_message = {
                    "role": "tool",
                    "content": result_content,
                    "tool_call_id": tool_call.id,
                }
                print_blue("action result:", result_content)

                messages.append(function_call_result_message)
            if i == max_iterations and response.choices[0].message.tool_calls != None:
                print_red("Max iterations reached")
                return "The tool agent could not complete the task in the given time. Please try again."
        print(vega_spec)

        final_prompt = {
                "role": "system",
                "content": '''Provide your final answer to the user naturally. Do not put any Vega specs, code, or weird formatting in your answer.
            
                '''
        }
        messages.append(final_prompt)
        final_response = client.chat.completions.create(
                model="gpt-4o-mini", temperature=0.0, messages=messages, tools=tools
            )
        description = final_response.choices[0].message.content
        if vega_spec and description:
                return QueryResponse(response=APIResponse(spec=vega_spec, description=description))
        elif description:
            return QueryResponse(response=APIResponse(description=description))
        else:
            return QueryResponse(response=APIResponse(description="No response generated."))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def print_red(*strings):
    print("\033[91m" + " ".join(strings) + "\033[0m")


# print msg in blue, , accept multiple strings like print statement
def print_blue(*strings):
    print("\033[94m" + " ".join(strings) + "\033[0m")


def delete_existing_files():
    for filename in os.listdir(UPLOAD_DIRECTORY):
        file_path = os.path.join(UPLOAD_DIRECTORY, filename)
        if os.path.isfile(file_path):
            os.remove(file_path)
            print(f"Deleted existing file: {file_path}")

UPLOAD_DIRECTORY = "static/uploads"
@app.post("/upload")
async def upload_file(file: UploadFile, request: Request):
    print("file", file)
    if not os.path.exists(UPLOAD_DIRECTORY):
        os.makedirs(UPLOAD_DIRECTORY)

    file_path = os.path.join(UPLOAD_DIRECTORY, file.filename)

    delete_existing_files()

    # Save the file to the static directory
    with open(file_path, "wb") as f:
        f.write(await file.read())

    # Dynamically get the host and scheme from the Request object
    host_url = request.url.scheme + "://" + request.headers["host"]

    # Construct the file URL dynamically
    file_url = f"{host_url}/static/uploads/{file.filename}"

    print("file_url", file_url)

    return JSONResponse({"file_url": file_url})

# Root endpoint
@app.get("/")
async def read_root():
    return FileResponse('static/index.html')
